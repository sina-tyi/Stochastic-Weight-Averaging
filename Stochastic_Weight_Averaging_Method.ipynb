{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stochastic Weight Averaging Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPQR14Ymd7-Q"
      },
      "source": [
        "\"\"\" TF-Keras SWA: callback utility for performing stochastic weight averaging (SWA).\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65_9XG2jd9t6"
      },
      "source": [
        "class SWA(Callback):\n",
        "    \"\"\" Stochastic Weight Averging.\n",
        "\n",
        "    # Arguments\n",
        "        start_epoch:   integer, epoch when swa should start.\n",
        "        lr_schedule:   string, type of learning rate schedule.\n",
        "        swa_lr:        float, learning rate for swa sampling.\n",
        "        swa_lr2:       float, upper bound of cyclic learning rate.\n",
        "        swa_freq:      integer, length of learning rate cycle.\n",
        "        verbose:       integer, verbosity mode, 0 or 1.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 start_epoch,\n",
        "                 lr_schedule='manual',\n",
        "                 swa_lr='auto',\n",
        "                 swa_lr2='auto',\n",
        "                 swa_freq=1,\n",
        "                 verbose=0):\n",
        "                 \n",
        "        super(SWA, self).__init__()\n",
        "        self.start_epoch = start_epoch - 1\n",
        "        self.lr_schedule = lr_schedule\n",
        "        self.swa_lr = swa_lr\n",
        "        self.swa_lr2 = swa_lr2\n",
        "        self.swa_freq = swa_freq\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if start_epoch < 2:\n",
        "            raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n",
        "\n",
        "        schedules = ['manual', 'constant', 'cyclic']\n",
        "\n",
        "        if self.lr_schedule not in schedules:\n",
        "            raise ValueError('\"{}\" is not a valid learning rate schedule' \\\n",
        "                             .format(self.lr_schedule))\n",
        "\n",
        "        if self.lr_schedule == 'cyclic' and self.swa_freq < 2:\n",
        "            raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n",
        "\n",
        "        if self.swa_lr == 'auto' and self.swa_lr2 != 'auto':\n",
        "            raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.') \n",
        "            \n",
        "        if self.lr_schedule == 'cyclic' and self.swa_lr != 'auto' \\\n",
        "           and self.swa_lr2 != 'auto' and self.swa_lr > self.swa_lr2:\n",
        "            raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9-3ytAieMba"
      },
      "source": [
        "def on_train_begin(self, logs=None):\n",
        "\n",
        "        self.epochs = self.params.get('epochs')\n",
        "\n",
        "        if self.start_epoch >= self.epochs - 1:\n",
        "            raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n",
        "\n",
        "        self.init_lr = K.eval(self.model.optimizer.lr)\n",
        "\n",
        "        # automatic swa_lr\n",
        "        if self.swa_lr == 'auto':\n",
        "            self.swa_lr = 0.1*self.init_lr\n",
        "        \n",
        "        if self.init_lr < self.swa_lr:\n",
        "            raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n",
        "\n",
        "        # automatic swa_lr2 between initial lr and swa_lr   \n",
        "        if self.lr_schedule == 'cyclic' and self.swa_lr2 == 'auto':\n",
        "            self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr)*0.25\n",
        "\n",
        "        self._check_batch_norm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7DUL-5eRWO"
      },
      "source": [
        "def on_epoch_begin(self, epoch, logs=None):\n",
        "\n",
        "        self.current_epoch = epoch\n",
        "        self._scheduler(epoch)\n",
        "\n",
        "        # constant schedule is updated epoch-wise\n",
        "        if self.lr_schedule == 'constant' or self.is_batch_norm_epoch:\n",
        "            self._update_lr(epoch)\n",
        "\n",
        "        if self.is_swa_start_epoch:\n",
        "            self.swa_weights = self.model.get_weights()\n",
        "\n",
        "            if self.verbose > 0:\n",
        "                print('\\nEpoch %05d: starting stochastic weight averaging'\n",
        "                      % (epoch + 1))\n",
        "\n",
        "        if self.is_batch_norm_epoch:\n",
        "            self._set_swa_weights(epoch)\n",
        "\n",
        "            if self.verbose > 0:\n",
        "                print('\\nEpoch %05d: reinitializing batch normalization layers'\n",
        "                      % (epoch + 1))\n",
        "\n",
        "            self._reset_batch_norm()\n",
        "\n",
        "            if self.verbose > 0:\n",
        "                print('\\nEpoch %05d: running forward pass to adjust batch normalization'\n",
        "                      % (epoch + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7eDQD8reUvB"
      },
      "source": [
        "def on_batch_begin(self, batch, logs=None):\n",
        "\n",
        "        # update lr each batch for cyclic lr schedule\n",
        "        if self.lr_schedule == 'cyclic':\n",
        "            self._update_lr(self.current_epoch, batch)\n",
        "\n",
        "        if self.is_batch_norm_epoch:\n",
        "\n",
        "            batch_size = self.params['samples']\n",
        "            momentum = batch_size / (batch*batch_size + batch_size)\n",
        "\n",
        "            for layer in self.batch_norm_layers:\n",
        "                layer.momentum = momentum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLfJz4oeeYeq"
      },
      "source": [
        " def on_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.eval(self.model.optimizer.lr)\n",
        "        for k, v in logs.items():\n",
        "            if k == 'lr':\n",
        "                self.model.history.history.setdefault(k, []).append(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CgiqNfbebKj"
      },
      "source": [
        "def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        if self.is_swa_start_epoch:\n",
        "            self.swa_start_epoch = epoch\n",
        "\n",
        "        if self.is_swa_epoch and not self.is_batch_norm_epoch:\n",
        "            self.swa_weights = self._average_weights(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOzEPADpedxJ"
      },
      "source": [
        "def on_train_end(self, logs=None):\n",
        "\n",
        "        if not self.has_batch_norm:\n",
        "            self._set_swa_weights(self.epochs)\n",
        "        else:\n",
        "            self._restore_batch_norm()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxV5ZdmegN5"
      },
      "source": [
        "    def _scheduler(self, epoch):\n",
        "\n",
        "        swa_epoch = (epoch - self.start_epoch)\n",
        "\n",
        "        self.is_swa_epoch = epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0\n",
        "        self.is_swa_start_epoch = epoch == self.start_epoch\n",
        "        self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm\n",
        "\n",
        "    def _average_weights(self, epoch):\n",
        "\n",
        "        return [(swa_w * (epoch - self.start_epoch) + w)\n",
        "                / ((epoch - self.start_epoch) + 1)\n",
        "                for swa_w, w in zip(self.swa_weights, self.model.get_weights())]\n",
        "\n",
        "    def _update_lr(self, epoch, batch=None):\n",
        "\n",
        "        if self.is_batch_norm_epoch:\n",
        "            lr = 0\n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "        elif self.lr_schedule == 'constant':\n",
        "            lr = self._constant_schedule(epoch)\n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "        elif self.lr_schedule == 'cyclic':\n",
        "            lr = self._cyclic_schedule(epoch, batch)\n",
        "            K.set_value(self.model.optimizer.lr, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ss6chRpeoy_"
      },
      "source": [
        " def _constant_schedule(self, epoch):\n",
        "\n",
        "        t = epoch / self.start_epoch\n",
        "        lr_ratio = self.swa_lr / self.init_lr\n",
        "        if t <= 0.5:\n",
        "            factor = 1.0\n",
        "        elif t <= 0.9:\n",
        "            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
        "        else:\n",
        "            factor = lr_ratio\n",
        "        return self.init_lr * factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svmeQqaqetqV"
      },
      "source": [
        " def _cyclic_schedule(self, epoch, batch):\n",
        "        \"\"\" Designed after Section 3.1 of Averaging Weights Leads to\n",
        "        Wider Optima and Better Generalization\n",
        "        \"\"\"\n",
        "        # steps are mini-batches per epoch, equal to training_samples / batch_size\n",
        "        steps = self.params.get('steps')\n",
        "        \n",
        "        #occasionally steps parameter will not be set. We then calculate it ourselves\n",
        "        if steps == None:\n",
        "            steps = self.params['samples'] // self.params['batch_size']\n",
        "        \n",
        "        swa_epoch = (epoch - self.start_epoch) % self.swa_freq\n",
        "        cycle_length = self.swa_freq * steps\n",
        "\n",
        "        # batch 0 indexed, so need to add 1\n",
        "        i = (swa_epoch * steps) + (batch + 1)\n",
        "        if epoch >= self.start_epoch:\n",
        "            t = (((i-1) % cycle_length) + 1)/cycle_length\n",
        "            return (1-t)*self.swa_lr2 + t*self.swa_lr\n",
        "        else:\n",
        "            return self._constant_schedule(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkgafIPtewzT"
      },
      "source": [
        "def _set_swa_weights(self, epoch):\n",
        "\n",
        "        self.model.set_weights(self.swa_weights)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print('\\nEpoch %05d: final model weights set to stochastic weight average'\n",
        "                  % (epoch + 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMTX79Pke9jt"
      },
      "source": [
        " def _check_batch_norm(self):\n",
        "\n",
        "        self.batch_norm_momentums = []\n",
        "        self.batch_norm_layers = []\n",
        "        self.has_batch_norm = False\n",
        "        self.running_bn_epoch = False\n",
        "\n",
        "        for layer in self.model.layers:\n",
        "            if issubclass(layer.__class__, BatchNormalization):\n",
        "                self.has_batch_norm = True\n",
        "                self.batch_norm_momentums.append(layer.momentum)\n",
        "                self.batch_norm_layers.append(layer)\n",
        "\n",
        "        if self.verbose > 0 and self.has_batch_norm:\n",
        "            print('Model uses batch normalization. SWA will require last epoch '\n",
        "                  'to be a forward pass and will run with no learning rate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsQqYOkvfBkU"
      },
      "source": [
        "def _reset_batch_norm(self):\n",
        "\n",
        "        for layer in self.batch_norm_layers:\n",
        "\n",
        "            # to get properly initialized moving mean and moving variance weights\n",
        "            # we initialize a new batch norm layer from the config of the existing\n",
        "            # layer, build that layer, retrieve its reinitialized moving mean and\n",
        "            # moving var weights and then delete the layer\n",
        "            bn_config = layer.get_config()\n",
        "            new_batch_norm = BatchNormalization(**bn_config)\n",
        "            new_batch_norm.build(layer.input_shape)\n",
        "            new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n",
        "            # get rid of the new_batch_norm layer\n",
        "            del new_batch_norm\n",
        "            # get the trained gamma and beta from the current batch norm layer\n",
        "            trained_weights = layer.get_weights()\n",
        "            new_weights = []\n",
        "            # get gamma if exists\n",
        "            if bn_config['scale']:\n",
        "                new_weights.append(trained_weights.pop(0))\n",
        "            # get beta if exists\n",
        "            if bn_config['center']:\n",
        "                new_weights.append(trained_weights.pop(0))\n",
        "            new_weights += [new_moving_mean, new_moving_var]\n",
        "            # set weights to trained gamma and beta, reinitialized mean and variance\n",
        "            layer.set_weights(new_weights)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKy1VmdqfEeF"
      },
      "source": [
        "def _restore_batch_norm(self):\n",
        "\n",
        "        for layer, momentum in zip(self.batch_norm_layers, self.batch_norm_momentums):\n",
        "            layer.momentum = momentum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYh93h0YfGok"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}